{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader,TensorDataset, Subset\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "import random\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device = ',device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 31.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 1, 625)\n",
      "(68, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_features(ppg_data, segment_length = 625):\n",
    "    num_segments = int(len(ppg_data) / segment_length)\n",
    "    features = []\n",
    "    for i in range(num_segments):\n",
    "        segment = ppg_data[i*segment_length : (i+1)*segment_length]\n",
    "        features.append(segment.values.flatten())\n",
    "    return np.array(features)\n",
    "\n",
    "# Load data\n",
    "# 读取文件列表\n",
    "ppg_files = glob.glob('ft_data/PPG_data/*.csv')\n",
    "bp_files = glob.glob('ft_data/BP_data/*.csv')\n",
    "n = 1\n",
    "k = random.randint(0, len(ppg_files)-n)\n",
    "ppg_files = ppg_files[k:k+n]\n",
    "bp_files = bp_files[k:k+n]\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "y_scalers = []\n",
    "\n",
    "i = 0\n",
    "for ppg_file, bp_file in tqdm(zip(ppg_files, bp_files), total = len(ppg_files)):\n",
    "        ppg_data = pd.read_csv(ppg_file)\n",
    "        bp_data = pd.read_csv(bp_file)\n",
    "        # print(ppg_data.shape)\n",
    "        # print(bp_data.shape)\n",
    "        \n",
    "        #ppg提取特征并分段\n",
    "        ppg_segments = create_features(ppg_data['PPG']) # (num_segments, 625)\n",
    "        # print(ppg_segments.shape)\n",
    "\n",
    "        #裁剪BP数据\n",
    "        y_sbp = bp_data['SBP'][:len(ppg_segments)]\n",
    "        # # 去除NaN值\n",
    "        # has_nan = np.isnan(y_sbp).any() \n",
    "        # if ((has_nan) == True):\n",
    "        #     print(f'文件{ppg_file}存在NaN值,跳过')\n",
    "\n",
    "        y_dbp = bp_data['DBP'][:len(ppg_segments)]\n",
    "        # print(y_sbp.shape, y_dbp.shape)\n",
    "\n",
    "        #调整数据形状\n",
    "        X = ppg_segments.reshape(-1, 1, 625)\n",
    "        y = np.column_stack((y_sbp, y_dbp))\n",
    "        \n",
    "        #添加到列表\n",
    "        X_list.extend(X)\n",
    "        y_list.extend(y)\n",
    "\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUBPModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUBPModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 定义 CNN 层\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=50, kernel_size=7, padding=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=50, out_channels=50, kernel_size=7, padding=3)\n",
    "        self.conv3 = nn.Conv1d(in_channels=50, out_channels=50, kernel_size=7, padding=3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "        self.bn3 = nn.BatchNorm1d(50)\n",
    "        # GRU 层前的 Batch Normalization\n",
    "        self.gru_bn = nn.BatchNorm1d(input_size)\n",
    "        # GRU 层后的 Batch Normalization\n",
    "        self.gru_out_bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # 定义 GRU 层\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # 定义全连接层\n",
    "        self.fc1 = nn.Linear(625 * hidden_size, 64)\n",
    "        self.fc1_bn = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 通过CNN层\n",
    "        conv1_out = self.conv1(x)\n",
    "        conv1_out = torch.relu(self.bn1(conv1_out))\n",
    "        \n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv2_out = torch.relu(self.bn2(conv2_out))\n",
    "        \n",
    "        conv3_out = self.conv3(conv2_out)\n",
    "        conv3_out = torch.relu(self.bn3(conv3_out))\n",
    "        \n",
    "        # lstm_input = conv1_out + conv3_out\n",
    "        x = torch.cat((conv1_out, conv3_out), dim=1) #(batch_size, 100, 625)\n",
    "        x= torch.transpose(x, 1, 2) # (batch_size, 625, 100)\n",
    "\n",
    "        # 在 LSTM 之前进行 Batch Normalization\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.gru_bn(x)# input (batch_size, 100, 625)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        \n",
    "        # 初始化 GRU 的隐藏状态\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # 通过GRU层\n",
    "        out, _ = self.gru(x, (h0))# input(batch_size, seq_len, input_size)\n",
    "\n",
    "        # 在 GRU之后进行 Batch Normalization\n",
    "        out = out.transpose(1, 2)\n",
    "        out = self.gru_out_bn(out)\n",
    "        out = out.transpose(1, 2)\n",
    "        \n",
    "        # 展平操作\n",
    "        out = out.contiguous().view(out.size(0), -1) # [batch_size, seq_len * hidden_size]\n",
    "\n",
    "        # 通过全连接层\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(self.fc1_bn(out))\n",
    "        out = self.fc2(out)\n",
    "        # out = torch.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "input_size = 100\n",
    "hidden_size = 25\n",
    "num_layers = 1\n",
    "output_size = 2\n",
    "\n",
    "model = GRUBPModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loo(model, train_loader, val_loader, scaler_y, criterion, optimizer, epochs):\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_avg_loss = running_loss / len(train_loader)\n",
    "        #print('Epoch: {}/{}, Average Train Loss: {:.6f}'.format(epoch+1, epochs, train_avg_loss))\n",
    "        train_loss_list.append(train_avg_loss)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        val_avg_loss, SBP_errors, DBP_errors = evaluate(model, val_loader, scaler_y, criterion)\n",
    "        #print('Average Val Loss: {:.6f}'.format(val_avg_loss))\n",
    "        #print(SBP_errors, DBP_errors)\n",
    "        val_loss_list.append(val_avg_loss)\n",
    "\n",
    "    \n",
    "    return train_loss_list, val_loss_list, SBP_errors, DBP_errors\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, scaler_y, criterion): #在函数外计算loss\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    SBP_pred_list = []\n",
    "    DBP_pred_list = []\n",
    "    SBP_ture_list = []\n",
    "    DBP_ture_list = []\n",
    "\n",
    "    SBP_errors = []\n",
    "    DBP_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            y_pred.append(outputs)\n",
    "            y_true.append(targets)\n",
    "\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    val_loss /= len(val_loader) # 求得单次val的平均loss\n",
    "\n",
    "    # 逆标准化\n",
    "    y_pred = scaler_y.inverse_transform(y_pred.cpu().numpy()) \n",
    "    y_true = scaler_y.inverse_transform(y_true.cpu().numpy())\n",
    "    #print('y_pred:', y_pred)\n",
    "\n",
    "    SBP_pred_list = np.concatenate((SBP_pred_list, y_pred[:,0]), axis=0)\n",
    "    SBP_ture_list= np.concatenate((SBP_ture_list, y_true[:,0]), axis=0)\n",
    "\n",
    "    DBP_pred_list = np.concatenate((DBP_pred_list, y_pred[:,1]), axis=0)\n",
    "    DBP_ture_list= np.concatenate((DBP_ture_list, y_true[:,1]), axis=0)\n",
    "\n",
    "    SBP_errors=(mean_absolute_error(SBP_ture_list,SBP_pred_list))\n",
    "    DBP_errors=(mean_absolute_error(DBP_ture_list, DBP_pred_list))\n",
    "\n",
    "    return val_loss, SBP_errors, DBP_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOO Cross Validation: 100%|██████████| 68/68 [00:12<00:00,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SBP Error: 4.618418\n",
      "Mean DBP Error: 1.654856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 留一交叉验证\n",
    "# 对模型参数进行微调\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "all_SBP_errors = []\n",
    "all_DBP_errors = []\n",
    "\n",
    "#标准化\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(X.reshape(-1, 625)).reshape(-1, 625, 1)\n",
    "X = X.transpose(0, 2, 1)\n",
    "y = scaler_y.fit_transform(y)\n",
    "\n",
    "\n",
    "for train_idx, val_idx in tqdm(loo.split(X), total = len(X), desc='LOO Cross Validation'):\n",
    "    #划分数据集\n",
    "    X_train_np, X_val_np = X[train_idx], X[val_idx]\n",
    "    y_train_np, y_val_np = y[train_idx], y[val_idx]\n",
    "\n",
    "    X_train = torch.tensor(X_train_np, dtype=torch.float32).to(device)\n",
    "    X_val = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train_np, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val_np, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 使用dataloader加载数据\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 创建模型实例\n",
    "    model = GRUBPModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "    # 加载预训练模型参数\n",
    "    model.load_state_dict(torch.load('pretraind_model_params1.pth'))\n",
    "    #model.load_state_dict(torch.load('checkpoint/model_epoch_20.pth'))\n",
    "    \n",
    "    # 冻结参数，只解冻需要微调的层\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'conv3' in name or 'fc2' in name or 'bn' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "    # 训练和验证\n",
    "    train_loss_list, val_loss_list, SBP_errors, DBP_errors = train_loo(model, train_loader, val_loader, scaler_y, criterion, optimizer, num_epochs)\n",
    "\n",
    "    all_SBP_errors.append(SBP_errors)\n",
    "    all_DBP_errors.append(DBP_errors)\n",
    "\n",
    "# 打印交叉验证结果\n",
    "\n",
    "mean_SBP_error = np.mean(all_SBP_errors)\n",
    "mean_DBP_error = np.mean(all_DBP_errors)\n",
    "\n",
    "print(f'Mean SBP Error: {mean_SBP_error:.6f}')\n",
    "print(f'Mean DBP Error: {mean_DBP_error:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOO Cross Validation: 100%|██████████| 68/68 [00:02<00:00, 30.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SBP Error: 5.116440\n",
      "Mean DBP Error: 1.918725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 留一交叉验证\n",
    "# 不经微调的原模型参数\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "all_SBP_errors = []\n",
    "all_DBP_errors = []\n",
    "\n",
    "\n",
    "for train_idx, val_idx in tqdm(loo.split(X), total = len(X), desc='LOO Cross Validation'):\n",
    "    #划分数据集\n",
    "    X_train_np, X_val_np = X[train_idx], X[val_idx]\n",
    "    y_train_np, y_val_np = y[train_idx], y[val_idx]\n",
    "\n",
    "    X_train = torch.tensor(X_train_np, dtype=torch.float32).to(device)\n",
    "    X_val = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train_np, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val_np, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 使用dataloader加载数据\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 创建模型实例\n",
    "    model = GRUBPModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "    # 加载预训练模型参数\n",
    "    model.load_state_dict(torch.load('pretraind_model_params1.pth'))\n",
    "    # model.load_state_dict(torch.load('checkpoint/model_epoch_20.pth'))\n",
    "    \n",
    "    # 冻结参数，只解冻需要微调的层\n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # 训练和验证\n",
    "    # train_loss_list, val_loss_list, SBP_errors, DBP_errors = train_loo(model, train_loader, val_loader, scaler_y, criterion, optimizer, num_epochs)\n",
    "    val_avg_loss, SBP_errors,DBP_errors = evaluate(model, val_loader, scaler_y, criterion)\n",
    "\n",
    "    all_SBP_errors.append(SBP_errors)\n",
    "    all_DBP_errors.append(DBP_errors)\n",
    "\n",
    "# 打印交叉验证结果\n",
    "\n",
    "mean_SBP_error = np.mean(all_SBP_errors)\n",
    "mean_DBP_error = np.mean(all_DBP_errors)\n",
    "\n",
    "print(f'Mean SBP Error: {mean_SBP_error:.6f}')\n",
    "print(f'Mean DBP Error: {mean_DBP_error:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
